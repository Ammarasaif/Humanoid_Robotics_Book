---
title: "Module 04: Vision-Language-Action (VLA) - Introduction"
description: "Introduction to Vision-Language-Action frameworks and LLM integration with robotics for voice-controlled humanoid operations"
sidebar_label: "Introduction"
---

# Module 04: Vision-Language-Action (VLA) - Introduction

Welcome to Module 04 of the Physical AI & Humanoid Robotics textbook. This module covers Vision-Language-Action (VLA) frameworks and their integration with Large Language Models (LLMs) for voice-controlled humanoid operations.

## Learning Objectives

After completing this module, you will be able to:

- Understand the fundamentals of Vision-Language-Action frameworks
- Implement voice recognition and speech-to-action pipelines using OpenAI Whisper
- Use LLMs for cognitive planning to translate natural language commands to ROS 2 actions
- Execute robot commands in simulation with path planning, obstacle navigation, and object recognition
- Integrate all VLA components into a complete autonomous humanoid system

## Navigation

- [Next: Chapter 01 - Introduction to VLA](./chapter-01-introduction.md)
- [Module 03: The AI-Robot Brain (NVIDIA Isaacâ„¢)](../module-03/intro.md)
- [Module 05: Next Module](#) (Coming Soon)